{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data\n",
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "We will use `CountVectorizer` to **convert text into a matrix of token count**.\n",
    "\n",
    "`Bag of Words`: https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "\n",
    "`Code Example`: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/  \n",
    "\n",
    "**We are going to perform below mentioned steps to understand the entire process:**  \n",
    "a. Converting text to numerical vectors with the help of `CountVectorizer`  \n",
    "b. Understand `fit` and `transform`  \n",
    "c. Looking at `vocabulary_`  \n",
    "d. Converting sparse matrix to dense matrix using `toarray()`  \n",
    "e. Understanding `n_gram`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Lets create 'lst_text' that will contain four doucuments\n",
    "lst_text = ['it was the best of times', 'it was the worst of times',\n",
    "            'it was the age of wisdom', 'it was the age of foolishness']\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vocab = CountVectorizer()\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "# dtm = vocab.fit_transform(lst_text)\n",
    "\n",
    "# fit_transform() could be done seperatly as mentioned below\n",
    "vocab.fit(lst_text)\n",
    "dtm = vocab.transform(lst_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 3,\n",
       " 'was': 7,\n",
       " 'the': 5,\n",
       " 'best': 1,\n",
       " 'of': 4,\n",
       " 'times': 6,\n",
       " 'worst': 9,\n",
       " 'age': 0,\n",
       " 'wisdom': 8,\n",
       " 'foolishness': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at unique words by using 'vocabulary_'\n",
    "\n",
    "vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Observe that the type of dtm is sparse\n",
    "\n",
    "print(type(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n"
     ]
    }
   ],
   "source": [
    "# Lets now print the  shape of this dtm\n",
    "\n",
    "print(dtm.shape)\n",
    "\n",
    "# o/p -> (4, 10)\n",
    "# i.e -> 4 documents and 10 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 9)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the dtm\n",
    "\n",
    "print(dtm)\n",
    "\n",
    "# Remember that dtm is a sparse matrix. i.e. zeros wont be stored\n",
    "# Lets understand First line of output -> (0,6)    1\n",
    "# Here (0, 6) means 0th document and 6th(index starting from 1) unique word. \n",
    "# (we have total 4 documents) & (we have total 10 unique words)\n",
    "# (0, 6)    1 -> 1 here refers to the number of occurence of 6th word\n",
    "# Now lets read it all in english.\n",
    "# (0, 6)    1 -> 'times' occurs 1 time in 0th document. \n",
    "# Try to observe -> (3, 3)   1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 1 1 1 0 0]\n",
      " [0 0 0 1 1 1 1 1 0 1]\n",
      " [1 0 0 1 1 1 0 1 1 0]\n",
      " [1 0 1 1 1 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Since the dtm is sparse, lets convert it into numpy array.\n",
    "\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-grams\n",
    "\n",
    "vocab = CountVectorizer(ngram_range=[1,2])\n",
    "\n",
    "dtm = vocab.fit_transform(lst_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 5, 'was': 16, 'the': 11, 'best': 2, 'of': 7, 'times': 15, 'it was': 6, 'was the': 17, 'the best': 13, 'best of': 3, 'of times': 9, 'worst': 19, 'the worst': 14, 'worst of': 20, 'age': 0, 'wisdom': 18, 'the age': 12, 'age of': 1, 'of wisdom': 10, 'foolishness': 4, 'of foolishness': 8}\n"
     ]
    }
   ],
   "source": [
    "print(vocab.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1]\n",
      " [1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0]\n",
      " [1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(dtm.toarray()) \n",
    "\n",
    "# convert sparse matrix to nparray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- `vect.fit(lst_text)` **learns the vocabulary**\n",
    "- `vect.transform(lst_text)` **uses the fitted vocabulary** to build a **document-term matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>subject enron methanol meter follow note gave ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>subject hpl nom januari see attach file hplnol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>subject neon retreat ho ho ho around wonder ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>subject photoshop window offic cheap main tren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>subject indian spring deal book teco pvr reven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                    clean_text_stem\n",
       "0   ham  subject enron methanol meter follow note gave ...\n",
       "1   ham  subject hpl nom januari see attach file hplnol...\n",
       "2   ham  subject neon retreat ho ho ho around wonder ti...\n",
       "3  spam  subject photoshop window offic cheap main tren...\n",
       "4   ham  subject indian spring deal book teco pvr reven..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/text_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.710114\n",
       "spam    0.289886\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = {'ham' : 0, 'spam' : 1}\n",
    "\n",
    "df['label_num'] = df['label'].apply(lambda x : label_encoder[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into test and train\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_text = []\n",
    "for text in train['clean_text_stem']:\n",
    "    train_clean_text.append(text)\n",
    "\n",
    "test_clean_text = []\n",
    "for text in test['clean_text_stem']:\n",
    "    test_clean_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "train_features = vectorizer.fit_transform(train_clean_text)\n",
    "\n",
    "test_features = vectorizer.transform(test_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 218908\n",
      "Type of train features: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of input data: (4136, 218908)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique words:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "print(\"Type of train features:\", type(train_features))\n",
    "\n",
    "print(\"Shape of input data:\", train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_features, train['label_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9758454106280193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(pred, test['label_num']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(train_features, train['label_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9458937198067633\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(pred, test['label_num']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(train_features, train['label_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9758454106280193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(pred, test['label_num']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
